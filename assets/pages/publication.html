<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Publications of C4G @ HKUST</title>

  <meta name="author" content="Wenhan Luo">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="icon" href="favicon.ico">
</head>

<body>


  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <center>
                      <h1>Publications of C4G @ HKUST</h1>
                    </center>
                  </p>
                  <p style="text-align:center">
                    (* indicates equal contribution, # indicates correspondence)
                  </p>


                </td>
              </tr>
            </tbody>
          </table>


          <br><br>
        </td>
      </tr>



  </table>

  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>UNIC: Unified In-Context Video Editing</b></papertitle>,
          <br>Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen,
          Wenhan Luo,<br>
          <i>International Conference on Learning Representations (ICLR), 2026.</i>
          <br>
          [<a href="https://arxiv.org/abs/2506.04216" target="_blank">arXiv</a>]
          [<a href="https://zixuan-ye.github.io/UNIC/" target="_blank">Project Page</a>]
          <p></p>
        </td>
      </tr>
      
    <tr>
     <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle><b>Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation</b></papertitle>,
                  <br>Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, Wenhan Luo,<br>
                  <i>Neural Information Processing Systems (NeurIPS), 2025.</i>
                  <br>
                  [<a href="https://arxiv.org/abs/2505.22647" target="_blank">arXiv</a>]
                  [<a href="https://meigen-ai.github.io/multi-talk/" target="_blank">Project Page</a>]
                  [<a href="https://github.com/MeiGen-AI/MultiTalk/" target="_blank">Code</a>]
                  [<a href="https://huggingface.co/MeiGen-AI/MeiGen-MultiTalk" target="_blank">Hugging Face Model</a>]
                  [<a href="https://huggingface.co/spaces/fffiloni/Meigen-MultiTalk" target="_blank">Gradio</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/MeiGen-AI/MultiTalk?style=social">
                  <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle><b>Foundation Cures Personalization: Improving Personalized Models' Prompt Consistency via Hidden Foundation Knowledge</b></papertitle>,
                  <br>Yiyang Cai, Zhengkai Jiang, Yulong Liu, Chunyang Jiang, Wei Xue, Yike Guo, Wenhan Luo,<br>
                  <i>Neural Information Processing Systems (NeurIPS), 2025.</i>
                  <br>
                  [<a href="https://arxiv.org/abs/2411.15277" target="_blank">PDF</a>]
                  [<a href="https://freecure.github.io/" target="_blank">Project Page</a>]
                  [<a href="https://github.com/YIYANGCAI/FreeCure" target="_blank">Code</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/YIYANGCAI/FreeCure?style=social">
         <p></p>
      </td>
    </tr>

      
      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>MaterialMVP: Illumination-Invariant Material Generation via Multi-view PBR Diffusion</b>
          </papertitle>,
          <br>Zebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao Zhang, Guanying Chen, Yuhong Liu, Jie
          Jiang, Chunchao Guo, Wenhan Luo#,<br>
          <i>Proc. of International Conference on Computer Vision (ICCV), Hawaii, USA, 2025. (<strong><font color="red">Highlight</font></strong>)</i>
          <br>
          [<a href="https://arxiv.org/abs/2503.10289" target="_blank">arXiv</a>]
          [<a href="https://zebinhe.github.io/MaterialMVP/" target="_blank">Project Page</a>]
          [<a href="https://github.com/ZebinHe/MaterialMVP" target="_blank">Code</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/ZebinHe/MaterialMVP?style=social">
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>MOERL: When Mixture-of-Experts Meet Reinforcement Learning for Adverse Weather Image
              Restoration</b></papertitle>,
          <br>Tao Wang, Peiwen Xia, Bo Li, Peng-Tao Jiang, Zhe Kong, Kaihao Zhang, Tong Lu, Wenhan Luo#,<br>
          <i>Proc. of International Conference on Computer Vision (ICCV), Hawaii, USA, 2025.</i>
          <br>
          [<a href="" target="_blank">PDF</a>]
          [<a href="" target="_blank">Code</a>]
          <p></p>
        </td>
      </tr>

      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution</b></papertitle>,
          <br>Zhe Kong, Le Li, Yong Zhang#, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming
          Wei, Guanying Chen, Wenhan Luo#,<br>
          <i>ACM SIGGRAPH, 2025.</i>
          <br>
          [<a href="https://arxiv.org/abs/2507.01012" target="_blank">PDF</a>]
          [<a href="https://kongzhecn.github.io/projects/dam-vsr/" target="_blank">Project Page</a>]
          [<a href="https://github.com/kongzhecn/DAM-VSR" target="_blank">Code</a>]
           <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/kongzhecn/DAM-VSR?style=social">
          <p></p>
        </td>
      </tr>

      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>StyleMaster: Stylize Your Video with Artistic Generation and Translation</b></papertitle>,
          <br>Zixuan Ye, Huijuan Huang#, Xintao Wang, Pengfei Wan, Di Zhang, Wenhan Luo#,<br>
          <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2025.</i>
          <br>
          [<a href="https://arxiv.org/pdf/2412.07744" target="_blank">arXiv</a>]
          [<a href="https://github.com/KwaiVGI/StyleMaster" target="_blank">Github</a>]
          [<a href="https://zixuan-ye.github.io/stylemaster/" target="_blank">Project Page</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/KwaiVGI/StyleMaster?style=social">
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Towards Multiple Character Image Animation Through Enhancing Implicit Decoupling</b>
          </papertitle>,
          <br>Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao
          Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo#,<br>
          <i>International Conference on Learning Representations (ICLR), 2025.</i>
          <br>
          [<a href="https://openreview.net/forum?id=aqlzXgXwWa" target="_blank">PDF</a>]
          [<a href="https://multi-animation.github.io/" target="_blank">Project Page</a>]
          [<a href="https://cloud.tencent.com/product/vclm" target="_blank">API in Tencent Cloud</a>]
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts</b></papertitle>,
          <br>Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, Min Zhang,<br>
          <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 47, pp. 3424-3439, 2025</i>
          <br>
          [<a href="https://arxiv.org/abs/2405.11273" target="_blank">arXiv</a>]
          [<a href="https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs" target="_blank">Code</a>]
          [<a href="https://uni-moe.github.io/" target="_blank">Project Page</a>]
          [<a
            href="https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs?tab=readme-ov-file#%EF%B8%8F-uni-moe-weights"
            target="_blank">Model</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs?style=social"><br>
          <a href="https://trendshift.io/repositories/10407" target="_blank"><img
              src="https://trendshift.io/api/badge/repositories/10407"
              alt="HITsz-TMG%2FUMOE-Scaling-Unified-Multimodal-LLMs | Trendshift" style="width: 200px; height: 44px;"
              width="200" height="44" /></a>
          <p></p>
        </td>
      </tr>


      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>APPTracker+: Displacement Uncertainty for Occlusion Handling in Low-Frame-Rate Multiple Object
              Tracking</b></papertitle>,
          <br>Tao Zhou, Qi Ye, Wenhan Luo, Haizhou Ran, Zhiguo Shi, Jiming Chen,<br>
          <i>International Journal of Computer Vision (IJCV), vol. 133, pp. 2044â€“2069, 2025.</i>
          <br>
          [<a href="https://link.springer.com/article/10.1007/s11263-024-02237-x" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Dual Teacher Knowledge Distillation with Domain Alignment for Face Anti-spoofing</b>
          </papertitle>,
          <br>Zhe Kong, Wentian Zhang, Tao Wang, Kaihao Zhang, Yuexiang Li, Xiaoying Tang, Wenhan Luo#,<br>
          <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 13177-13189, 2024.</i>
          <br>
          [<a href="https://arxiv.org/abs/2401.01102" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Blind Face Video Restoration with Temporal Consistent Generative Prior and Degradation-Aware
              Prompt</b></papertitle>,
          <br>Jingfan Tan, Hyunhee Park, Ying Zhang, Tao Wang, Kaihao Zhang, Xiangyu Kong, Pengwen Dai, Zikun Liu,
          Wenhan Luo#,<br>
          <i>The 32rd ACM International Conference on Multimedia (ACM MM), 2024.</i>
          <br>
          [<a href="https://dl.acm.org/doi/10.1145/3664647.3680917" target="_blank">PDF</a>]
          <p></p>

        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>OMG: Occlusion-friendly Personalized Multi-concept Generation In Diffusion Models</b>
          </papertitle>,
          <br>Zhe Kong, Yong Zhang#, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, Wenhan
          Luo#,<br>
          <i>European Conference on Computer Vision (ECCV), 2024.</i>
          <br>
          [<a href="https://arxiv.org/abs/2403.10983" target="_blank">PDF</a>]
          [<a href="https://github.com/kongzhecn/OMG" target="_blank">Code</a>]
          [<a href="https://kongzhecn.github.io/omg-project/" target="_blank">Project Page</a>]
          [<a href="https://huggingface.co/spaces/Fucius/OMG" target="_blank">Hugging Face (OMG+LoRAs)</a>]
          [<a href="https://huggingface.co/spaces/Fucius/OMG-InstantID" target="_blank">Hugging Face
            (OMG+InstantID)</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/kongzhecn/OMG?style=social">
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Prompting Future Driven Diffusion Model for Hand Motion Prediction</b></papertitle>,
          <br>Bowen Tang, Kaihao Zhang#, Wenhan Luo#, Wei Liu, Hongdong Li,<br>
          <i>European Conference on Computer Vision (ECCV), 2024.</i>
          <br>
          [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01102.pdf" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>


      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse
              Weather Conditions</b></papertitle>,
          <br>Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo#, Bjorn Stenger, Tong Lu, Tae-Kyun Kim, Wei Liu,
          Hongdong Li,<br>
          <i>International Journal of Computer Vision (IJCV), vol. 132, pp. 4541-4563, 2024.</i>
          <br>
          [<a href="https://arxiv.org/abs/2305.17863" target="_blank">PDF</a>]
          [<a href="https://github.com/TaoWangzj/GridFormer" target="_blank">Code</a>]
          <p></p>
        </td>
      </tr>





      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer</b>
          </papertitle>,
          <br>Jingfan Tan, Xiaoxu Chen, Tao Wang, Kaihao Zhang, Wenhan Luo#, Xiaochun Cao,<br>
          <i>IEEE Trans. on Circuits and Systems for Video Technology (TCSVT), vol. 34, pp. 4914-4927, 2024.</i>
          <br>
          [<a href="https://arxiv.org/abs/2308.10196" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>


      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Punctuation-level Attack: Single-shot and Single Punctuation Can Fool Text Models</b>
          </papertitle>,
          <br>Wenqiang Wang, Chongyang Du, Tao Wang, Kaihao Zhang, Wenhan Luo#, Lin Ma, Wei Liu, Xiaochun Cao,<br>
          <i>Neural Information Processing Systems (NeurIPS), 2023.</i>
          <br>
          [<a href="https://openreview.net/pdf?id=ir6WWkFR80" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>





      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Taming Self-Supervised Learning for Presentation Attack Detection: De-Folding and De-Mixing</b>
          </papertitle>,
          <br>Zhe Kong, Wentian Zhang, Feng Liu, Wenhan Luo, Haozhe Liu, Linlin Shen, Raghavendra Ramachandra,<br>
          <i>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), vol. 35, pp. 10639-10650, 2024.</i>
          <br>
          [<a href="https://arxiv.org/abs/2109.04100" target="_blank">PDF</a>]
          [<a href="https://github.com/kongzhecn/dfdm" target="_blank">Code</a>]
          <p></p>
        </td>
      </tr>


      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Ultra-High-Defnition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method</b>
          </papertitle>,
          <br>Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo#, Bjorn Stenger, Tong Lu#,<br>
          <i>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), USA, 2023. (<strong><font color="red">Oral</font></strong>)</i>
          <br>
          [<a href="https://arxiv.org/abs/2212.11548" target="_blank">PDF</a>]
          [<a href="https://github.com/TaoWangzj/LLFormer" target="_blank">Code</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/TaoWangzj/LLFormer?style=social">
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Multiple Object Tracking: A Literature Review</b></papertitle>,
          <br>Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Tae-Kyun. Kim,<br>
          <i>Artificial Intelligence, vol. 293, pp. 103448, 2021. (<strong><font color="red">Highly Cited Paper</font></strong>)</i>
          <br>
          [<a href="https://www.sciencedirect.com/science/article/pii/S0004370220301958" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>

      <tr>

        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Face Anti-Spoofing: Model Matters, So Does Data</b></papertitle>,
          <br>
          Xiao Yang*, Wenhan Luo*, Linchao Bao, Yuan Gao, Dihong Gong, Shibao Zheng, Zhifeng Li, Wei Liu,
          <br>
          <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), USA, 2019.</i>
          <br>
          [<a
            href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Face_Anti-Spoofing_Model_Matters_so_Does_Data_CVPR_2019_paper.pdf"
            target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>


      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning</b>
          </papertitle>,
          <br>Wenhan Luo*, Peng Sun*, Fangwei Zhong*, Wei Liu, Tong Zhang, Yizhou Wang,<br>
          <i>IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 42, pp. 1317-1332, 2020.</i>
          <br>
          [<a href="https://arxiv.org/abs/1808.03405" target="_blank">arXiv</a>]
          [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018" target="_blank">Project
            Page</a>]
          [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018" target="_blank">Code</a>]
          [<a href="https://github.com/zfw1226/gym-unrealcv" target="_blank">Environment</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social">
          <p></p>
        </td>
      </tr>


      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Trajectories as Topics: Multi-Object Tracking by Topic Discovery</b></papertitle>,
          <br>Wenhan Luo, Bjorn Stenger, Xiaowei Zhao, Tae-Kyun Kim,<br>
          <i>IEEE Trans. on Image Processing (TIP), vol. 28, no. 1, pp. 240-252, 2019.</i>
          <br>
          [<a href="https://ieeexplore.ieee.org/document/8444759" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>


      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>End-to-end Active Object Tracking via Reinforcement Learning</b></papertitle>,
          <br>Wenhan Luo*, Peng Sun*, Fangwei Zhong, Wei Liu, Tong Zhang, Yizhou Wang,<br>
          <i>International Conference on Machine Learning (ICML), Sweden, 2018.</i>
          <br>
          [<a href="https://arxiv.org/abs/1705.10561" target="_blank">PDF</a>]
          [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018" target="_blank">Project
            Page</a>]
          [<a href="https://sites.google.com/site/whluoimperial/active_tracking_icml2018" target="_blank">Code</a>]
          [<a href="https://drive.google.com/file/d/19Tz2rfF6i1CcTonOoS-xy1nIgx1qcQ9x/view" target="_blank">Demo</a>]
          <p></p>
        </td>
      </tr>

      <tr>
        <td style="padding:0px;width:70%;vertical-align:middle;text-align:justify">
          <papertitle><b>Automatic Topic Discovery for Multi-object Tracking</b></papertitle>,
          <br>Wenhan Luo, Bjorn Stenger, Xiaowei Zhao, Tae-Kyun Kim,<br>
          <i>Proc. of the Association for the Advancement of Artificial Intelligence (AAAI), Austin, Texas, USA,
            2015. (<strong><font color="red">Oral</font></strong>)</i>
          <br>
          [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/9789" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>

      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Bi-label Propagation for Generic Multiple Object Tracking</b></papertitle>,
          <br>Wenhan Luo, Tae-Kyun Kim, Bjorn Stenger, Xiaowei Zhao, Roberto Cipolla,<br>
          <i>Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, USA, 2014.</i>
          <br>
          [<a href="https://ieeexplore.ieee.org/document/6909564" target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>

      <tr style="padding:0px;text-align: justify;">
        <td style="padding:0px">
          <papertitle><b>Generic Object Crowd Tracking by Multi-Task Learning</b></papertitle>,<br>
          Wenhan Luo, Tae-Kyun Kim,<br>
          <i>Proc. of British Machine Vision Conference (BMVC), Bristol, UK, 2013.</i><br>
          [<a
            href="https://www.researchgate.net/publication/269250315_Generic_Object_Crowd_Tracking_by_Multi-Task_Learning"
            target="_blank">PDF</a>]
          <p></p>
        </td>
      </tr>



    </tbody>
  </table>



  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <br>
      <tr style="padding:0px">
        <td style="padding:0px">
          <div class="section">
            <h2>Tech Report</h2>
        </td>

      </tr>

      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling</b></papertitle>,
          <br>Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke, Didan Deng, Han Gao, Yongxiang Huang, Kaihao Zhang, Hongbo Fu, Wenhan Luo,<br>
          <i>arXiv:2602.11146.</i>
          <br>
          [<a href="https://arxiv.org/abs/2602.11146" target="_blank">arXiv</a>]
          [<a href="https://github.com/HKUST-C4G/diffusion-rm/" target="_blank">Code</a>]
          <p></p>
        </td>
      </tr>

      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</b></papertitle>,
          <br>Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo,<br>
          <i>arXiv:2512.19686.</i>
          <br>
          [<a href="https://arxiv.org/abs/2512.19686" target="_blank">arXiv</a>]
          [<a href="https://zixuan-ye.github.io/VACoT/" target="_blank">Project Page</a>]
          <p></p>
        </td>
      </tr>
      
      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement</b></papertitle>,
          <br>Zhizhou Zhong, Yicheng Ji, Zhe Kong, Yiying Liu, Jiarui Wang, Jiasun Feng, Lupeng Liu, Xiangyi Wang, Yanjia Li, Yuqing She, Ying Qin, Huan Li, Shuiyang Mao, Wei Liu, Wenhan Luo,<br>
          <i>	arXiv:2511.23475.</i>
          <br>
          [<a href="https://arxiv.org/abs/2511.23475" target="_blank">arXiv</a>]
          [<a href="https://hkust-c4g.github.io/AnyTalker-homepage/" target="_blank">Project Page</a>]
          [<a href="https://github.com/HKUST-C4G/AnyTalker" target="_blank">Code</a>]
          [<a href="https://huggingface.co/spaces/C4G-HKUST/AnyTalker" target="_blank">Gradio</a>]
          [<a href="https://huggingface.co/zzz66/AnyTalker-1.3B/tree/main" target="_blank">Hugging Face Model</a>]
          <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/HKUST-C4G/AnyTalker?style=social">         
          <p></p>
        </td>
      </tr>
      




      <tr>
       <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
        <papertitle><b>InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing</b></papertitle>,
                  <br>Shaoshu Yang*, Zhe Kong*, Feng Gao*, Meng Cheng*, Xiangyu Liu*, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, Xiaoming Wei,<br>
                  <i>arXiv:2508.14033.</i>
                  <br>
                  [<a href="https://arxiv.org/abs/2508.14033" target="_blank">arXiv</a>]
                  [<a href="https://meigen-ai.github.io/InfiniteTalk/" target="_blank">Project Page</a>]
                  [<a href="https://github.com/MeiGen-AI/InfiniteTalk" target="_blank">Code</a>]
                  <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/MeiGen-AI/InfiniteTalk?style=social">
                  <p></p>
      </td>
    </tr>


      <tr>
        <td style="padding:0px;width:65%;vertical-align:middle;text-align:justify">
          <papertitle><b>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</b>
          </papertitle>,
          <br>Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang,
          Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao
          Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, Min Zhang,<br>
          <i>arXiv:2505.04921, 2025.</i>
          <br>
          [<a href="https://arxiv.org/abs/2505.04921" target="_blank">arXiv</a>]
          [<a href="https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models/" target="_blank">Github</a>]
          <img alt="GitHub stars" style="vertical-align:middle"
            src="https://img.shields.io/github/stars/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models?style=social">
          <p></p>
        </td>
      </tr>












    </tbody>
  </table>





</body>

</html>
